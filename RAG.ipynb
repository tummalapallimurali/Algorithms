{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUUZwRYRm0LDJQIhT5weyS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tummalapallimurali/Algorithms/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use case to read ML paper titles which are lengthy for general audience. They can be too technical, so i will be using open source RAG to generate short titles based on previously created short titles.\n",
        "\n",
        "# Libraries used\n",
        "- Chroma DB vector store DB\n",
        "- Sentence transformer - To generate emebeddings.\n",
        "- pandas to read csv file.\n",
        "- Open AI Key is optional to compare query results with RAG responses.\n",
        "\n",
        "# Prompting Instructions\n"
      ],
      "metadata": {
        "id": "hsEzuyIfHK1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CReSHLsHGKkD"
      },
      "outputs": [],
      "source": [
        "# install libraries:\n",
        "\n",
        "%%capture\n",
        "!pip install chromadb sentence-transformers pandas python-dotenv openai\n",
        "\n",
        "import json\n",
        "import chromadb\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ml_papers = pd.read_csv(\"/content/ml-potw-10232023.csv\", header=0, encoding=\"utf-8\")\n",
        "\n",
        "# remove empty titles or description\n",
        "\n",
        "ml_papers = ml_papers.dropna(subset=[\"Title\", \"Description\"])\n",
        "\n",
        "# read title and description to json format\n",
        "\n",
        "ml_papers_dict = ml_papers.to_dict(orient=\"records\")\n",
        "\n",
        "ml_papers_dict[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZPOJ4rMVKL",
        "outputId": "c7774bd4-6925-4ddb-b79f-ab8228fa6ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Title': 'Llemma',\n",
              " 'Description': 'an LLM for mathematics which is based on continued pretraining from Code Llama on the Proof-Pile-2 dataset; the dataset involves scientific paper, web data containing mathematics, and mathematical code; Llemma outperforms open base models and the unreleased Minerva on the MATH benchmark; the model is released, including dataset and code to replicate experiments.',\n",
              " 'PaperURL': 'https://arxiv.org/abs/2310.10631',\n",
              " 'TweetURL': 'https://x.com/zhangir_azerbay/status/1714098025956864031?s=20',\n",
              " 'Abstract': 'We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing , Embeddings and Storing documents in vector database,"
      ],
      "metadata": {
        "id": "A--W3Mm8ORfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                      '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')\n",
        "\n",
        "# initialize chroma db directory and client\n",
        "client = chromadb.PersistentClient(path=\"/content/chromadb\")\n",
        "\n",
        "# create collections\n",
        "\n",
        "collection = client.get_or_create_collection(name=\"ml_papers\")\n",
        "\n",
        "# create batch embeddings function\n",
        "\n",
        "# generate embedings in batches\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# loop and add embeddings to vector store\n",
        "\n",
        "for i in tqdm(range(0, len(ml_papers_dict), batch_size)):\n",
        "    batch = ml_papers_dict[i:i+batch_size]\n",
        "\n",
        "    # if empty string is found in title, mark as \"No title\"\n",
        "\n",
        "    batch_titles = [str(paper['Title']) if str(paper['Title'] !=\"\") else \"No title\" for paper in batch]\n",
        "\n",
        "    # generate random batch_ids for each batch_titles\n",
        "\n",
        "    batch_ids = [str(sum(ord(c) + random.randint(1, 10000) for c in paper[\"Title\"])) for paper in batch]\n",
        "\n",
        "    # generate embeddings for batch_titles\n",
        "\n",
        "    batch_metadata = [paper['PaperURL'] for paper in batch]\n",
        "\n",
        "    embeddings = embedding_model.encode(batch_titles)\n",
        "\n",
        "    # insert into vector DB\n",
        "\n",
        "    collection.upsert(\n",
        "        documents=batch_titles,\n",
        "        embeddings=embeddings.tolist(),\n",
        "        ids=batch_ids\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkVatSQVOlVA",
        "outputId": "4e1574eb-5240-4f61-a212-1e9730d2f954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:03<00:00,  2.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Retriever\n",
        "\n",
        "retriver = collection.query(\n",
        "    query_texts=[\"Software Engineering\"],\n",
        "    n_results=2,\n",
        ")\n",
        "\n",
        "print(retriver[\"documents\"])\n",
        "print(retriver[\"distances\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGgrbPLfgn1X",
        "outputId": "dbf23916-998f-48ea-8dcd-ee095c528973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['LLMs for Software Engineering', 'LLMs for Software Engineering']]\n",
            "[[0.8221281170845032, 0.8221281170845032]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y1N-nO6nUKQ",
        "outputId": "4965d3ee-0e0e-4b8d-89a6-6f11320d856e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
            "ChemCrow: Augmenting large-language models with chemistry tools\n",
            "A Survey of Large Language Models\n",
            "LLaMA: Open and Efficient Foundation Language Models\n",
            "SparseGPT: Massive Language Models Can Be Accurately Pruned In One-Shot\n",
            "REPLUG: Retrieval-Augmented Black-Box Language Models\n",
            "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n",
            "Auditing large language models: a three-layered approach\n",
            "Fine-Tuning Language Models with Just Forward Passes\n",
            "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# get chat completions\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def get_completion(prompt, model = \"gpt-3.5-turbo\"):\n",
        "  client = OpenAI(api_key=userdata.get('Open_AI_Key') )\n",
        "  completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0,\n",
        "    max_tokens=50,\n",
        "\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pOm3RcbyHXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Models\"\n",
        "\n",
        "# query for user query\n",
        "results = collection.query(\n",
        "    query_texts=[user_query],\n",
        "    n_results=10,\n",
        ")\n",
        "\n",
        "short_titles = '\\n'.join(results['documents'][0])\n",
        "print(short_titles)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h6FGfr7LTKh",
        "outputId": "1335abe9-4d8a-4067-b07a-b68c173760f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
            "ChemCrow: Augmenting large-language models with chemistry tools\n",
            "A Survey of Large Language Models\n",
            "LLaMA: Open and Efficient Foundation Language Models\n",
            "SparseGPT: Massive Language Models Can Be Accurately Pruned In One-Shot\n",
            "REPLUG: Retrieval-Augmented Black-Box Language Models\n",
            "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n",
            "Auditing large language models: a three-layered approach\n",
            "Fine-Tuning Language Models with Just Forward Passes\n",
            "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt templates\n",
        "\n",
        "prompt_template =  f'''[INS]\n",
        "\n",
        "your main task is to generate 5 SUGGESTED_PAPER_TITLE based on PAPER_TITLE\n",
        "\n",
        "you should mimic a similar style and length as short_titles\n",
        "\n",
        "PAPER_TITLE :{user_query}\n",
        "SHORT_TITLES :{short_titles}\n",
        "SUGGESTED_PAPER_TITLE :\n",
        "\n",
        "[/INS]\n",
        "'''\n",
        "\n",
        "results = collection.query(\n",
        "    query_texts=[prompt_template],\n",
        "    n_results=10,\n",
        ")\n",
        "\n",
        "short_titles = '\\n'.join(results['documents'][0])\n",
        "print(short_titles)\n",
        "\n",
        "# responses = get_completion(prompt_template)\n",
        "# suggested_titles = ''.join([str(r) for r in responses])\n",
        "\n",
        "# print(suggested_titles)\n",
        "\n",
        "# Model Suggestions:\n",
        "\n",
        "# 1. S3Eval: A Comprehensive Evaluation Suite for Large Language Models\n",
        "# 2. Synthetic and Scalable Evaluation for Large Language Models\n",
        "# 3. Systematic Evaluation of Large Language Models with S3Eval\n",
        "# 4. S3Eval: A Synthetic and Scalable Approach to Language Model Evaluation\n",
        "# 5. S3Eval: A Synthetic and Scalable Evaluation Suite for Large Language Models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3JvM6a6NLV_",
        "outputId": "15cd4d12-9a58-491e-af1e-93a3fad28da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChemCrow: Augmenting large-language models with chemistry tools\n",
            "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\n",
            "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents\n",
            "LLaMA: Open and Efficient Foundation Language Models\n",
            "SparseGPT: Massive Language Models Can Be Accurately Pruned In One-Shot\n",
            "Eight Things to Know about Large Language Models\n",
            "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\n",
            "A Survey of Large Language Models\n",
            "REPLUG: Retrieval-Augmented Black-Box Language Models\n",
            "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "\n",
        "The short titles generated by LLM can be improved using fine-tuning techniques, overall RAG is critical to productionize the Gen AI projects."
      ],
      "metadata": {
        "id": "PW6nkBtFNcEj"
      }
    }
  ]
}